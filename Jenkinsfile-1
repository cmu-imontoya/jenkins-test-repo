pipeline {
    agent any

    environment {
        SONARQUBE_URL = "http://34.138.255.10:9000"            // Replace with SonarQube server URL
        SONARQUBE_TOKEN = credentials('sonar')           // Jenkins credentials for SonarQube token
        DATAPROC_CLUSTER = "project-option1"                   // Replace with your Dataproc cluster name
        GCP_REGION = "us-east1"                                // Replace with the Dataproc cluster region
        GCS_BUCKET = "dataproc-bucket-11-14-2024"              // Replace with your GCS bucket for staging files
    }

    stages {
        stage('Checkout Code') {
            steps {
                git url: 'https://github.com/cmu-imontoya/jenkins-test-repo/', branch: 'main'
            }
        }

        stage('SonarQube Analysis') {
            steps {
                // Using withSonarQubeEnv to set up the environment for SonarQube analysis
                withSonarQubeEnv(installationName: 'sonar') {
                    sh '''
                        mvn sonar:sonar \
                        -Dsonar.projectKey=your_project_key \
                        -Dsonar.host.url=$SONARQUBE_URL \
                        -Dsonar.login=$SONARQUBE_TOKEN
                    '''
                }
            }
        }

        stage('Quality Gate Check') {
            steps {
                script {
                    // Wait for SonarQube analysis to complete and check the quality gate
                    def qualityGate = waitForQualityGate()
                    if (qualityGate.status != 'OK') {
                        error "SonarQube Quality Gate failed with status: ${qualityGate.status}"
                    } else {
                        echo 'SonarQube Analysis Completed'
                    }
                }
            }
        }

        stage('Upload Scripts to GCS') {
            steps {
                // Using withGCP to authenticate with GCP
                withGCP(credentialsId: 'final-proj') {
                    script {
                        // Copy mapper and reducer scripts to Google Cloud Storage bucket
                        sh "gsutil cp mapper.py gs://$GCS_BUCKET/"
                        sh "gsutil cp reducer.py gs://$GCS_BUCKET/"
                    }
                }
            }
        }

        stage('Run Hadoop Job on Dataproc') {
            steps {
                // Using withGCP for submitting the Dataproc job
                withGCP(credentialsId: 'final-proj') {
                    script {
                        sh """
                            gcloud dataproc jobs submit hadoop \
                            --cluster=$DATAPROC_CLUSTER \
                            --region=$GCP_REGION \
                            --jar=file:///usr/lib/hadoop-mapreduce/hadoop-streaming.jar \
                            -- -files gs://$GCS_BUCKET/mapper.py,gs://$GCS_BUCKET/reducer.py \
                            -mapper 'python3 mapper.py' \
                            -reducer 'python3 reducer.py' \
                            -input gs://$GCS_BUCKET/input_data/ \
                            -output gs://$GCS_BUCKET/output_data/
                        """
                    }
                }
            }
        }
    }

    post {
        failure {
            echo 'Pipeline failed!'
        }
        success {
            echo 'Pipeline succeeded!'
        }
    }
}
