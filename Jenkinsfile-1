pipeline {
    agent any
    environment {
        HADOOP_HOME = '/usr/local/hadoop'
        HDFS_INPUT_PATH = '/data/input/'
        HDFS_OUTPUT_PATH = '/TempOutFolder'
        LOCAL_TEMP_RESULTS = '/tmp/tempResults'
        GITHUB_REPO = 'https://github.com/cmu-imontoya/jenkins-test-repo'
        SONARQUBE_URL = 'http://sonarqube:9000'
    }

    stages {
        stage('Git Checkout') {
            steps {
                checkout scmGit(branches: [[name: '*/main']], extensions: [], userRemoteConfigs: [[url: "${GITHUB_REPO}"]])
                echo 'Git Checkout Completed'
            }
        }

        stage('SonarQube Analysis') {
            steps {
                withSonarQubeEnv(installationName: 'sonar') {
                    script {
                        echo 'Starting SonarQube Analysis'
                        sh """
                            sonar-scanner \
                            -Dsonar.projectKey=jenkins-test-repo \
                            -Dsonar.sources=./ \
                            -Dsonar.host.url=${SONARQUBE_URL}
                        """
                    }
                }
            }
        }

        stage('Wait for SonarQube Analysis') {
            steps {
                script {
                    echo 'Waiting for SonarQube Analysis to finish...'
                    timeout(time: 1, unit: 'HOURS') {
                        waitForQualityGate abortPipeline: true
                    }
                }
            }
        }

        stage('Run Hadoop Job on Cloud Cluster') {
            when {
                expression {
                    return currentBuild.result == 'SUCCESS'
                }
            }
            steps {
                echo 'Running Hadoop MapReduce Job on Cloud Cluster'

                script {
                    echo 'Moving data to HDFS...'
                    sh """
                        hadoop fs -put /local/path/to/data/* ${HDFS_INPUT_PATH}
                    """
                }

                script {
                    echo 'Copying Mapper and Reducer to Hadoop Cluster...'
                    sh """
                        scp temperature_mapper.py user@hadoop_cluster:/path/to/mapper/
                        scp temperature_reducer.py user@hadoop_cluster:/path/to/reducer/
                    """
                }

                script {
                    echo 'Executing Hadoop MapReduce Job...'
                    sh """
                        hadoop jar ${HADOOP_HOME}/share/hadoop/tools/lib/hadoop-streaming-*.jar \
                            -file /path/to/mapper/temperature_mapper.py \
                            -mapper 'python /path/to/mapper/temperature_mapper.py' \
                            -file /path/to/reducer/temperature_reducer.py \
                            -reducer 'python /path/to/reducer/temperature_reducer.py' \
                            -input ${HDFS_INPUT_PATH} \
                            -output ${HDFS_OUTPUT_PATH}
                    """
                }

                script {
                    echo 'Retrieving results from Hadoop...'
                    sh """
                        hadoop fs -getmerge ${HDFS_OUTPUT_PATH} ${LOCAL_TEMP_RESULTS}
                    """
                    sh """
                        cat ${LOCAL_TEMP_RESULTS}
                    """
                }

                script {
                    echo 'Copying results to Cloud Bucket...'
                    sh """
                        gsutil cp ${LOCAL_TEMP_RESULTS} gs://my-bucket/tempResults/
                    """
                }
            }
        }

        stage('Clean Up') {
            steps {
                echo 'Cleaning up resources...'
                sh """
                    hadoop fs -rm -r ${HDFS_OUTPUT_PATH}
                    rm ${LOCAL_TEMP_RESULTS}
                """
            }
        }
    }

    post {
        success {
            echo 'Pipeline executed successfully!'
        }
        failure {
            echo 'Pipeline failed!'
        }
    }
}
